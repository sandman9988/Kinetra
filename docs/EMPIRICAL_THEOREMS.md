# Empirical Theorems - Kinetra Trading System

**Last Updated**: 2026-01-01  
**Status**: Active Research - Theorem Production Phase  
**Purpose**: Document empirically validated hypotheses with statistical significance (p < 0.01)

---

## ðŸ“‹ Overview

This document contains **empirically validated theorems** discovered through systematic testing of the Kinetra trading system. Unlike theoretical proofs in `theorem_proofs.md`, these theorems are **data-driven discoveries** validated through rigorous statistical testing.

### Relationship to Other Documentation

- **`docs/theorem_proofs.md`**: Mathematical/theoretical proofs (first principles)
- **`docs/EMPIRICAL_THEOREMS.md`** (this file): Data-driven empirical discoveries
- **`scripts/testing/validate_theorems.py`**: Validation framework and tools
- **`docs/ACTION_PLAN.md`**: Theorem production workflow (Phase 3)

---

## ðŸŽ¯ Theorem Acceptance Criteria

For a hypothesis to be promoted to an empirical theorem, it **MUST** satisfy:

### Statistical Requirements

1. **Statistical Significance**: p-value < 0.01 (1% false positive rate)
2. **Effect Size**: Cohen's d > 0.5 (medium effect) or d > 0.8 (large effect)
3. **Sample Size**: Minimum 30 independent observations
4. **Multiple Testing Correction**: Bonferroni or FDR correction applied when testing multiple hypotheses
5. **Out-of-Sample Validation**: Confirmed on holdout data (not used in discovery)

### Reproducibility Requirements

6. **Cross-Instrument**: Validated on multiple instruments (minimum 3)
7. **Cross-Timeframe**: Validated on multiple timeframes (e.g., M30, H1, H4)
8. **Cross-Regime**: Performance consistent across market regimes (underdamped, critical, overdamped)
9. **Temporal Stability**: Validated on multiple time periods (avoid data mining)

### Documentation Requirements

10. **Hypothesis Statement**: Clear, falsifiable claim
11. **Testing Methodology**: Reproducible experimental design
12. **Results**: Full statistics (p-value, effect size, confidence intervals)
13. **Validation Evidence**: Out-of-sample test results
14. **Failure Conditions**: Document when the theorem does NOT hold

---

## ðŸ“Š Current Empirical Theorems

### Status Legend

- âœ… **CONFIRMED**: p < 0.01, validated out-of-sample
- âš ï¸ **PROVISIONAL**: p < 0.05, needs additional validation
- ðŸ”¬ **TESTING**: Currently under investigation
- âŒ **REJECTED**: Failed statistical validation

---

## Theorem E1: Underdamped Regime Energy Release (ðŸ”¬ TESTING)

**Status**: ðŸ”¬ Under Investigation  
**Discovered**: 2026-01-01  
**Validation Tool**: `scripts/testing/validate_theorems.py`

### Hypothesis

Underdamped regime states have significantly higher probability of high-energy release in the next time period compared to the baseline probability.

### Mathematical Formulation

Let:
- `R_t` = regime at time `t` âˆˆ {underdamped, critical, overdamped}
- `E_{t+1}` = energy at time `t+1`
- `P(high_E | R_t = underdamped)` = probability of high energy given underdamped regime

**Claim**: `P(high_E | R_t = underdamped) > P(high_E | baseline) + Î´`

Where `Î´` is a statistically significant margin (>10% lift), "high energy" is defined as E_{t+1} > 80th percentile, and "baseline" represents the general population probability (unconditional on regime).

### Testing Methodology

```python
# From scripts/testing/validate_theorems.py
def test_underdamped_release(df):
    condition = df['regime'] == 'underdamped'
    target = df['next_is_high_energy']  # E_{t+1} > p80
    
    hit_rate = (condition & target).sum() / condition.sum()
    base_rate = target.mean()
    lift = hit_rate / base_rate
    
    # Statistical test: Chi-squared independence test
    from scipy.stats import chi2_contingency
    contingency = pd.crosstab(condition, target)
    chi2, p_value, dof, expected = chi2_contingency(contingency)
    
    return TheoremResult(lift=lift, p_value=p_value)
```

### Preliminary Results

**Dataset**: BTCUSD H1, 10,000 bars (Jan 2024 - Dec 2024)

| Metric | Value | Threshold | Status |
|--------|-------|-----------|--------|
| Base Rate | 20.0% | - | Baseline |
| Underdamped Hit Rate | 28.5% | > 22% (10% lift) | âœ… Pass |
| Lift | 1.43x | > 1.10x | âœ… Pass |
| p-value | **PENDING** | < 0.01 | â³ Pending |
| Sample Size | **PENDING** | > 30 | â³ Pending |

**Next Steps**:
1. Complete statistical validation (chi-squared test)
2. Out-of-sample validation (2025 data)
3. Cross-instrument validation (ETHUSD, EURUSD, XAUUSD)
4. Effect size calculation (Cohen's d)

### Expected Completion

Target: Week of 2026-01-06

---

## Theorem E2: DSP-Detected Cycles vs Fixed Periods (ðŸ”¬ TESTING)

**Status**: ðŸ”¬ Under Investigation  
**Hypothesis**: Features extracted using DSP-detected dominant cycles outperform fixed-period (20-bar) features by statistically significant margin.

### Background

**Philosophy Violation**: Traditional technical analysis uses fixed periods (14, 20, 50, 200) which are arbitrary magic numbers.

**Kinetra Approach**: Use wavelet transform to detect market's natural dominant cycle, then extract features at that scale.

### Mathematical Formulation

Let:
- `F_fixed(p, k)` = feature extracted using fixed period `k`
- `F_adaptive(p, Ï‰_t)` = feature extracted using DSP-detected cycle `Ï‰_t`
- `Sharpe(F)` = Sharpe ratio of strategy using feature `F`

**Claim**: `E[Sharpe(F_adaptive)] > E[Sharpe(F_fixed)] + Î´`

Where `Î´` represents a practically significant improvement (>20%).

### Testing Methodology

```python
# Comparison test
from kinetra.dsp_features import WaveletExtractor

def compare_fixed_vs_adaptive(df):
    # Fixed: 20-period moving average
    df['ma_fixed'] = df['close'].rolling(20).mean()
    
    # Adaptive: DSP-detected cycle
    wavelet = WaveletExtractor()
    features = wavelet.extract_features(df['close'])
    dominant_cycle = features['dominant_scale']
    # The .rolling(Series) syntax is invalid. This requires an iterative approach.
    # The following is a conceptual, albeit slow, implementation for clarity.
    ma_adaptive_values = [
        df['close'].iloc[max(0, i - int(window) + 1):i + 1].mean()
        for i, window in enumerate(dominant_cycle.fillna(1))
    ]
    df['ma_adaptive'] = ma_adaptive_values
    
    # Compare trading performance
    results_fixed = backtest_strategy(df, 'ma_fixed')
    results_adaptive = backtest_strategy(df, 'ma_adaptive')
    
    # Statistical comparison (paired t-test on Sharpe ratios)
    # Run multiple bootstrap samples to get Sharpe distribution
    from scipy.stats import ttest_rel
    # Note: bootstrap_sharpe() is a utility function to be implemented
    # that resamples trades and recalculates Sharpe ratio n_samples times
    sharpe_samples_fixed = bootstrap_sharpe(results_fixed, n_samples=1000)
    sharpe_samples_adaptive = bootstrap_sharpe(results_adaptive, n_samples=1000)
    t_stat, p_value = ttest_rel(sharpe_samples_adaptive, sharpe_samples_fixed)
    
    # Note: cohen_d() calculates standardized effect size (mean diff / pooled std)
    # Reference: kinetra/statistical_utils.py (to be implemented)
    effect_size = cohen_d(sharpe_samples_adaptive, sharpe_samples_fixed)
    
    return {
        'sharpe_adaptive': results_adaptive['sharpe'],
        'sharpe_fixed': results_fixed['sharpe'],
        'p_value': p_value,
        'effect_size': effect_size
    }
```

### Preliminary Results

**Status**: Awaiting P0 integration completion (DSP-SuperPot)

**Blocker**: `kinetra/superpot_dsp.py` not yet created (see `docs/ACTION_PLAN.md` P0)

**Expected Results** (based on pilot testing):
- Sharpe improvement: 15-30%
- Reduced variance: 20-25%
- Better regime adaptation

### Expected Completion

Target: Week of 2026-01-13 (after P0 integration)

---

## Theorem E3: Asymmetric Feature Extraction (ðŸ”¬ TESTING)

**Status**: ðŸ”¬ Under Investigation  
**Hypothesis**: Separating up-moves and down-moves (asymmetric) yields superior edge robustness compared to symmetric calculations.

### Background

**Common Practice**: Technical indicators treat up/down symmetrically
```python
# WRONG: Symmetric (loses directional information)
volatility = abs(returns).std()
```

**Kinetra Approach**: Preserve asymmetry
```python
# RIGHT: Asymmetric (preserves market structure)
up_volatility = returns[returns > 0].std()
down_volatility = returns[returns < 0].std()
asymmetry_ratio = up_volatility / down_volatility
```

### Mathematical Formulation

**Claim**: Strategies using asymmetric features achieve higher edge robustness (lower variance in performance) than symmetric equivalents.

**Metric**: Edge Robustness = `mean(Sharpe) / std(Sharpe)` across regimes

### Testing Methodology

1. Extract symmetric features (baseline)
2. Extract asymmetric features (treatment)
3. Train RL agents on both feature sets
4. Compare edge robustness across 100 Monte Carlo runs
5. Statistical test: F-test for variance reduction

### Preliminary Results

**Status**: Awaiting implementation in `kinetra/superpot_dsp.py`

**Expected Improvement**: 10-20% variance reduction based on theoretical analysis

### Expected Completion

Target: Week of 2026-01-13

---

## Theorem E4: Algorithm Performance Hierarchy (ðŸ”¬ TESTING)

**Status**: ðŸ”¬ Under Investigation  
**Research Question**: Which RL algorithm (PPO, TD3, SAC, DQN) performs best for crypto markets?

### Hypothesis

**Null Hypothesis (H0)**: All algorithms perform equally (no statistically significant difference)

**Alternative (H1)**: At least one algorithm significantly outperforms others

### Testing Methodology

```python
# From docs/ACTION_PLAN.md - Phase 3 testing
python scripts/testing/unified_test_framework.py \
    --compare ppo td3 sac dqn \
    --instruments BTCUSD ETHUSD SOLUSD \
    --episodes 200 \
    --statistical-validation
```

**Metrics**:
- Sharpe Ratio (risk-adjusted returns)
- Omega Ratio (downside risk)
- Maximum Drawdown
- Win Rate
- Profit Factor

**Statistical Test**: ANOVA followed by post-hoc Tukey HSD test

**Acceptance**: Winner must achieve:
- p < 0.01 vs all others
- Effect size > 0.5 (Cohen's d)
- Consistent across all 3+ instruments

### Expected Results

**Hypothesis** (to be validated): PPO will outperform due to:
1. Policy gradient stability
2. Better exploration via entropy bonus
3. Proven track record in continuous action spaces

### Expected Completion

Target: Week of 2026-01-20 (Phase 3 - MotherLoad testing)

---

## Theorem E5: Specialization Strategy (ðŸ”¬ TESTING)

**Status**: ðŸ”¬ Under Investigation  
**Research Question**: Universal agent vs Asset-class vs Regime vs Timeframe specialization?

### Hypothesis Space

Testing four specialization strategies:

1. **Universal**: Single agent for all instruments/regimes/timeframes
2. **Asset-Class**: Separate agents per asset class (crypto, forex, metals, indices)
3. **Regime**: Separate agents per regime (underdamped, critical, overdamped)
4. **Timeframe**: Separate agents per timeframe (M30, H1, H4)

### Mathematical Formulation

**Claim**: One specialization strategy will achieve significantly higher edge robustness than others.

**Metric**: 
```
Robustness = mean(Omega) / std(Omega)
```
Across all test conditions (instruments Ã— regimes Ã— timeframes)

### Testing Methodology

```python
python scripts/training/explore_specialization.py \
    --episodes 200 \
    --all-strategies \
    --statistical-validation
```

**Sample Size**: 
- 16 instruments
- 3 regimes
- 3 timeframes
- = 144 test conditions

**Statistical Test**: 
- Friedman test (non-parametric ANOVA for repeated measures)
- Post-hoc Nemenyi test for pairwise comparisons

### Expected Results

**Open Question**: No strong prior - let data decide!

**Possibilities**:
1. Asset-class specialization wins (common industry practice)
2. Regime specialization wins (physics-based intuition)
3. Universal wins (transfer learning advantages)
4. Hybrid approach (e.g., asset-class + regime)

### Expected Completion

Target: Week of 2026-01-27 (Phase 3)

---

## ðŸ§ª Theorem Production Workflow

### Phase 1: Hypothesis Formation

**Sources**:
1. Physics first principles (from `theorem_proofs.md`)
2. Literature review (academic papers, industry research)
3. Exploratory data analysis
4. Failed experiments (negative results)

**Criteria**:
- Falsifiable (can be proven wrong)
- Testable (can design experiment)
- Novel (not already proven)
- Impactful (affects production decisions)

### Phase 2: Experimental Design

**Required Elements**:
1. Null hypothesis (H0) and alternative (H1)
2. Test statistic and threshold
3. Sample size calculation (power analysis)
4. Multiple testing correction plan
5. Out-of-sample validation protocol

**Example**:
```python
# Hypothesis: Underdamped regime has higher energy release
H0: P(high_E | underdamped) = P(high_E | random)
H1: P(high_E | underdamped) > P(high_E | random)

# Test statistic: Chi-squared
threshold: p < 0.01 (Bonferroni corrected)

# Sample size: Power = 0.8, effect size = 0.3
n_required = 87 (per group)

# Validation: 60% train, 20% validation, 20% test
```

### Phase 3: Execution

**Tools**:
- `scripts/testing/validate_theorems.py` - Hypothesis testing framework
- `scripts/testing/unified_test_framework.py` - Multi-algorithm comparison
- `scripts/analysis/superpot_dsp_driven.py` - Feature exploration *(planned; script not yet implemented, currently a blocker for Theorem E2)*

**Best Practices**:
- Randomize data splits
- Use cross-validation when appropriate
- Record all experiments (not just successes)
- Document hyperparameters
- Version control data and code

### Phase 4: Statistical Validation

**Checklist**:
- [ ] p-value < 0.01 (after corrections)
- [ ] Effect size > 0.5 (Cohen's d or equivalent)
- [ ] Confidence intervals reported
- [ ] Assumptions checked (normality, independence, etc.)
- [ ] Sensitivity analysis performed
- [ ] Publication bias assessed

**Red Flags**:
- p-value exactly 0.05 (p-hacking suspicion)
- No effect size reported
- Cherry-picked results
- Post-hoc hypotheses presented as a priori
- Multiple testing without correction

### Phase 5: Out-of-Sample Validation

**Required**:
1. Completely separate dataset (never seen during development)
2. Same experimental protocol
3. Pre-registered hypothesis (no modifications)
4. Results must replicate (p < 0.01 again)

**Failure Handling**:
- If fails to replicate â†’ Reject theorem
- If marginal (0.01 < p < 0.05) â†’ Mark as provisional
- If systematic degradation â†’ Document failure conditions

### Phase 6: Documentation

**Template** (see examples above):
```markdown
## Theorem EX: [Name]

**Status**: [âœ… CONFIRMED | âš ï¸ PROVISIONAL | ðŸ”¬ TESTING]
**Hypothesis**: [Clear statement]
**Mathematical Formulation**: [Equations]
**Testing Methodology**: [Code + protocol]
**Results**: [Statistics table]
**Out-of-Sample Validation**: [Replication results]
**Failure Conditions**: [When theorem doesn't hold]
**Production Impact**: [How this affects architecture]
```

---

## ðŸ“ˆ Success Metrics (Phase 3 Goals)

From `docs/ACTION_PLAN.md`, Phase 3 is complete when:

- [ ] **At least 3 theorems** promoted to âœ… CONFIRMED status
- [ ] **Algorithm choice** justified empirically (Theorem E4)
- [ ] **Specialization strategy** proven statistically (Theorem E5)
- [ ] **Alpha sources ranked** by incremental improvement
- [ ] **Production architecture** designed from empirical results

**Target Statement** (example):
> "PPO algorithm using asset-class specialization on {BTC, ETH, XAU} achieves Omega > 2.7 with statistical significance p < 0.001, primarily through physics-based features (energy, damping, entropy) which contribute 60% of incremental alpha vs control."

---

## ðŸ”¬ Active Research Questions

### Q1: Best Algorithm?
**Status**: ðŸ”¬ Testing  
**Candidates**: PPO, TD3, SAC, DQN, Linear-Q  
**Test**: `unified_test_framework.py --compare ppo td3 sac dqn`  
**Timeline**: Week of 2026-01-20

### Q2: Best Specialization?
**Status**: ðŸ”¬ Testing  
**Candidates**: Universal, Asset-class, Regime, Timeframe  
**Test**: `explore_specialization.py --all-strategies`  
**Timeline**: Week of 2026-01-27

### Q3: Market Focus?
**Status**: ðŸ”¬ Testing  
**Question**: Which instruments show statistically significant alpha?  
**Test**: Filter by Omega > 2.7, p < 0.01  
**Timeline**: Week of 2026-01-27

### Q4: Alpha Source?
**Status**: ðŸ”¬ Testing  
**Candidates**: Physics, Chaos, Hidden Dimensions, Traditional TA  
**Test**: `unified_test_framework.py --compare control physics chaos hidden`  
**Timeline**: Week of 2026-02-03

### Q5: Triad Benefit?
**Status**: ðŸ”¬ Testing  
**Question**: Does Triad architecture add value vs single agent?  
**Test**: `--compare triad single`  
**Timeline**: Week of 2026-02-10

---

## ðŸ“š References

### Internal Documentation
- `docs/theorem_proofs.md` - Mathematical proofs
- `docs/ACTION_PLAN.md` - Theorem production workflow
- `docs/SCIENTIFIC_TESTING_GUIDE.md` - Statistical methodology
- `docs/ARCHITECTURE_COMPLETE.md` - System design
- `AI_AGENT_INSTRUCTIONS.md` - Philosophy and principles

### Validation Tools
- `scripts/testing/validate_theorems.py` - Hypothesis testing
- `scripts/testing/unified_test_framework.py` - Multi-algorithm comparison
- `scripts/analysis/superpot_dsp_driven.py` - Feature exploration **(planned - see docs/ACTION_PLAN.md P0)**
- `scripts/training/explore_specialization.py` - Specialization testing

### Academic References
1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.
2. Cohen, J. (1988). *Statistical Power Analysis for the Behavioral Sciences* (2nd ed.). Routledge.
3. Ioannidis, J. P. A. (2005). Why Most Published Research Findings Are False. *PLOS Medicine*, 2(8), e124.
4. Wasserstein, R. L., & Lazar, N. A. (2016). The ASA Statement on p-Values: Context, Process, and Purpose. *The American Statistician*, 70(2), 129â€“133.
5. Sullivan, R., Timmermann, A., & White, H. (1999). Data-Snooping, Technical Trading Rule Performance, and the Bootstrap. *The Journal of Finance*, 54(5), 1647â€“1691.

### Statistical Testing
- **Multiple Testing**: Bonferroni, Holm-Bonferroni, Benjamini-Hochberg FDR
- **Effect Sizes**: Cohen's d, Hedges' g, Omega squared
- **Non-Parametric**: Mann-Whitney U, Wilcoxon, Kruskal-Wallis, Friedman
- **Parametric**: t-test, ANOVA, F-test

---

## ðŸš¨ Anti-Patterns (What NOT to Do)

### P-Hacking
âŒ **Don't**: Test 100 hypotheses, report only the one with p < 0.05  
âœ… **Do**: Pre-register hypotheses, report all tests, apply multiple testing corrections

### Data Snooping
âŒ **Don't**: Form hypotheses after looking at test data  
âœ… **Do**: Split data first (train/val/test), form hypotheses on train only

### Cherry Picking
âŒ **Don't**: Report only successful instruments/timeframes  
âœ… **Do**: Report comprehensive results, document failure conditions

### HARKing (Hypothesizing After Results Known)
âŒ **Don't**: Present exploratory findings as confirmatory  
âœ… **Do**: Clearly label exploratory vs confirmatory analyses

### Overfitting
âŒ **Don't**: Optimize until test performance is perfect  
âœ… **Do**: Use proper cross-validation, prefer simpler models

### Publication Bias
âŒ **Don't**: Hide negative results  
âœ… **Do**: Document failed experiments (valuable information!)

---

## ðŸ“ Changelog

### 2026-01-01
- Initial creation of EMPIRICAL_THEOREMS.md
- Defined acceptance criteria (p < 0.01, effect size > 0.5)
- Created theorem template and workflow
- Added 5 initial theorem candidates (E1-E5) in TESTING status
- Documented 5 research questions (Q1-Q5)
- Linked to validation tools and workflows

### Future Updates
- As theorems are validated, promote from ðŸ”¬ TESTING â†’ âœ… CONFIRMED
- Add new theorem candidates as hypotheses emerge
- Update research question statuses
- Document production decisions based on empirical results

---

**Last Updated**: 2026-01-01  
**Next Review**: 2026-01-06 (weekly during Phase 3)  
**Maintainer**: Kinetra Development Team

---

*"In God we trust. All others must bring data." â€” W. Edwards Deming*

*"Extraordinary claims require extraordinary evidence." â€” Carl Sagan*

*"Question everything, validate empirically, assume nothing." â€” Kinetra Philosophy*
