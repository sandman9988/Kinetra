# Kinetra Integration Guide
## Connecting the Pieces: Testing Framework â†’ Alpha Discovery

**Purpose**: Step-by-step guide to wire up existing components  
**Status**: Integration plumbing tasks (research tooling phase)  
**Last Updated**: 2026-01-01

---

## ğŸ¯ Current State: Pieces Exist, Need Wiring

### âœ… What We Have

| Component | Status | Location |
|-----------|--------|----------|
| **Data Pipeline** | âœ… Complete | `kinetra/data_loader.py`, `data_package.py` |
| **87 Datasets** | âœ… Available | `data/master/*.csv` |
| **Physics Engine** | âœ… Complete | `kinetra/physics_engine.py` |
| **Testing Framework** | âœ… Core built | `kinetra/testing_framework.py` |
| **RL Agents** | âœ… Multiple implemented | `rl_agent.py`, `rl_neural_agent.py` |
| **Triad System** | âœ… Designed | `triad_system.py`, `doppelganger_triad.py` |
| **Risk Management** | âœ… Ready | `risk_management.py`, `tripleganger_system.py` |
| **Backtest Engine** | âœ… Complete | `backtest_engine.py` |

### âš ï¸ What Needs Wiring

| Integration Point | Status | Priority |
|-------------------|--------|----------|
| **DSP Features â†” SuperPot** | âŒ Legacy uses fixed periods | ğŸ”¥ **P0 - CRITICAL** |
| Testing Framework â†” RL Agents | âŒ Not connected | ğŸ”¥ P1 - CRITICAL |
| Discovery Methods â†” Testing | âŒ Stub implementations | ğŸ”¥ P2 - HIGH |
| Physics Engine â†” Test Environments | âš ï¸ Partially wired | ğŸ”¥ P2 - HIGH |
| Results â†” Analytics Dashboard | âŒ Manual analysis | ğŸŸ¡ P3 - MEDIUM |
| Training Scripts â†” Unified Interface | âš ï¸ Scattered | ğŸŸ¡ P4 - MEDIUM |

**âš ï¸ NEW P0 PRIORITY**: Replace fixed-period superpot with DSP-driven adaptive cycles

---

## ğŸ”Œ Integration Task 0: DSP-Driven Features â†’ SuperPot (P0 - CRITICAL)

### Current Gap

**Philosophy Violation**: Legacy superpot uses fixed periods (5, 10, 20 bars), violating core principle of "no fixed periods, no magic numbers"

```python
# scripts/analysis/superpot_explorer.py (CURRENT - WRONG):
features[fi] = np.mean(ret[-5:])   # Magic number 5!
features[fi] = np.mean(ret[-10:])  # Magic number 10!
features[fi] = np.mean(ret[-20:])  # Magic number 20!
```

### Integration Steps

#### Step 0.1: Create DSP-Driven Feature Extractor

```python
# File: kinetra/superpot_dsp.py (NEW)

from kinetra.dsp_features import WaveletExtractor, HilbertExtractor
from kinetra.assumption_free_measures import AsymmetricReturns

class DSPSuperPotExtractor:
    """
    SuperPot feature extraction using DSP-detected cycles.
    NO FIXED PERIODS. EVER.
    
    Replaces: scripts/analysis/superpot_explorer.py (legacy)
    """
    
    def __init__(self):
        self.wavelet = WaveletExtractor(min_scale=2, max_scale=64)
        self.hilbert = HilbertExtractor()
        self.feature_names = self._build_feature_names()
    
    def extract(self, df: pd.DataFrame, idx: int) -> np.ndarray:
        """Extract features using DSP-detected cycles (not fixed periods)."""
        
        if idx < 100:
            return np.zeros(len(self.feature_names))
        
        # Get price data
        prices = df['close'].values[:idx+1]
        
        # DSP: Detect market's natural cycles
        wavelet_features = self.wavelet.extract_features(prices)
        dominant_cycle = wavelet_features['dominant_scale']  # Market tells us!
        
        # Use detected cycle for all calculations (not 5, 10, 20)
        features = []
        
        # Price action over DETECTED cycle (not fixed 20)
        if dominant_cycle > 0:
            cycle_return = (prices[-1] / prices[-dominant_cycle] - 1)
        else:
            cycle_return = 0
        features.append(cycle_return)
        
        # ASYMMETRIC returns (up/down separate)
        asymm = AsymmetricReturns.extract_features(prices, lookback=dominant_cycle)
        features.extend([
            asymm['up_sum'],      # Total upside (positive)
            asymm['down_sum'],    # Total downside (negative) 
            # NEVER COMBINED!
        ])
        
        # Hilbert instantaneous frequency (not bar-count based)
        hilbert = self.hilbert.extract_features(prices)
        features.append(hilbert['frequency'])  # Actual market rhythm
        
        # Wavelet energy PER SCALE (multiple cycles, not just one)
        for scale in wavelet_features['energy'].keys():
            features.append(wavelet_features['energy'][scale])
        
        # DIRECTIONAL wavelet features (up/down separate)
        for scale in wavelet_features['energy'].keys():
            # Get positive and negative coefficients SEPARATELY
            # (never square and sum together - that's symmetric!)
            pass  # Implementation needed
        
        return np.array(features, dtype=np.float32)
    
    def _build_feature_names(self) -> List[str]:
        """Build feature names WITHOUT fixed periods."""
        return [
            'cycle_return',        # Over DETECTED cycle, not 20
            'up_sum',              # Asymmetric (separate)
            'down_sum',            # Asymmetric (separate)
            'inst_frequency',      # Hilbert (instantaneous)
            # ... wavelet energies per scale (data-driven)
            # ... directional features (asymmetric)
        ]
```

#### Step 0.2: Update SuperPot Scripts

```python
# File: scripts/analysis/superpot_dsp_driven.py (NEW)

from kinetra.superpot_dsp import DSPSuperPotExtractor

def run_dsp_superpot(instruments, episodes=100):
    """
    SuperPot with DSP-detected cycles.
    Replaces legacy superpot_explorer.py
    """
    
    extractor = DSPSuperPotExtractor()  # NO fixed periods!
    tracker = FeatureImportanceTracker(
        extractor.n_features,
        extractor.feature_names
    )
    
    # Same empirical discovery methodology
    # BUT: Features use adaptive cycles, not magic numbers
    
    for ep in range(episodes):
        # ... training loop ...
        
        # Prune based on performance (adaptive, not every 20)
        if tracker.should_prune():  # When improvement plateaus
            tracker.prune_bottom_performers(adaptive_count)
```

#### Step 0.3: Validation

```bash
# Compare legacy vs DSP-driven superpot
python scripts/analysis/superpot_explorer.py --episodes 50  # Legacy (fixed)
python scripts/analysis/superpot_dsp_driven.py --episodes 50  # New (adaptive)

# Verify:
# 1. DSP version has NO fixed periods (5, 10, 20) in code
# 2. DSP version uses dominant_scale from wavelets
# 3. DSP version separates up/down moves (asymmetric)
# 4. Both discover similar alpha (methodology works)
```

---

## ğŸ”Œ Integration Task 1: Testing Framework â†” RL Agents

### Current Gap

```python
# testing_framework.py has this:
def run_test_suite(config: TestConfiguration) -> TestResult:
    # ... setup ...
    
    # âŒ MISSING: How to instantiate and train RL agents
    if config.agent_type == 'rl':
        # TODO: Wire up RL training loop
        pass
```

### Integration Steps

#### Step 1.1: Create Agent Factory

```python
# File: kinetra/agent_factory.py (NEW)

from typing import Any, Dict, Type
from kinetra.rl_agent import KinetraAgent  # PPO
from kinetra.rl_neural_agent import NeuralAgent  # DQN
from kinetra.triad_system import IncumbentAgent, CompetitorAgent, ResearcherAgent

class AgentFactory:
    """Factory for creating RL agents from config."""
    
    AGENT_REGISTRY = {
        'ppo': KinetraAgent,
        'dqn': NeuralAgent,
        'linear_q': LinearQAgent,  # From rl_exploration_framework.py
        'incumbent': IncumbentAgent,
        'competitor': CompetitorAgent,
        'researcher': ResearcherAgent,
    }
    
    @classmethod
    def create(cls, agent_type: str, state_dim: int, action_dim: int, 
               config: Dict[str, Any]) -> Any:
        """Create agent instance from type string."""
        if agent_type not in cls.AGENT_REGISTRY:
            raise ValueError(f"Unknown agent type: {agent_type}")
        
        agent_class = cls.AGENT_REGISTRY[agent_type]
        
        # Handle different constructor signatures
        if agent_type in ['ppo', 'dqn']:
            return agent_class(
                state_dim=state_dim,
                action_dim=action_dim,
                **config
            )
        elif agent_type == 'linear_q':
            return agent_class(
                n_features=state_dim,
                n_actions=action_dim,
                **config
            )
        elif agent_type in ['incumbent', 'competitor', 'researcher']:
            # Triad agents need base algorithm
            base_algo = config.get('algorithm', 'ppo')
            base_agent = cls.create(base_algo, state_dim, action_dim, config)
            return agent_class(base_agent=base_agent, **config)
        
        return agent_class(**config)
```

#### Step 1.2: Wire into Testing Framework

```python
# File: kinetra/testing_framework.py (MODIFY)

from kinetra.agent_factory import AgentFactory
from rl_exploration_framework import TradingEnv  # Multi-instrument env

def run_rl_test(config: TestConfiguration) -> TestResult:
    """Run RL agent test suite."""
    
    # Create multi-instrument environment
    env = TradingEnv(
        instruments=config.instruments,
        physics_state_dim=64,
        use_gpu=config.use_gpu
    )
    
    # Create agent from config
    agent = AgentFactory.create(
        agent_type=config.agent_type,
        state_dim=env.observation_space.shape[0],
        action_dim=env.action_space.n,
        config=config.agent_config
    )
    
    # Training loop
    episode_rewards = []
    for episode in range(config.episodes):
        state = env.reset()
        done = False
        total_reward = 0
        
        while not done:
            # Agent selects action
            action = agent.select_action(state, epsilon=0.1)
            
            # Environment step
            next_state, reward, done, info = env.step(action)
            
            # Agent learns
            agent.update(state, action, reward, next_state, done)
            
            state = next_state
            total_reward += reward
        
        episode_rewards.append(total_reward)
        
        # Log progress
        if episode % 10 == 0:
            logger.info(f"Episode {episode}: Reward = {total_reward:.2f}")
    
    # Calculate metrics
    metrics = calculate_metrics(episode_rewards, env.get_trade_history())
    
    return TestResult(
        config=config,
        metrics=metrics,
        episode_rewards=episode_rewards,
        trade_history=env.get_trade_history()
    )
```

#### Step 1.3: Update Test Runner

```python
# File: kinetra/testing_framework.py (MODIFY)

def run_test_suite(config: TestConfiguration) -> TestResult:
    """Main test runner - routes to appropriate test type."""
    
    if config.agent_type in ['control', 'ma', 'rsi', 'macd']:
        return run_control_test(config)
    
    elif config.agent_type in ['ppo', 'dqn', 'linear_q', 'rl']:
        return run_rl_test(config)  # â† NEW
    
    elif config.agent_type == 'triad':
        return run_triad_test(config)  # â† NEW
    
    elif config.agent_type == 'physics':
        return run_physics_test(config)
    
    else:
        raise ValueError(f"Unknown agent type: {config.agent_type}")
```

---

## ğŸ”Œ Integration Task 2: Discovery Methods â†” Testing

### Current Gap

Discovery suites (chaos, hidden, meta, etc.) are documented but not implemented:

```python
# scripts/testing/unified_test_framework.py has:
AVAILABLE_SUITES = [
    'control', 'physics', 'rl', 'specialization', 'stacking', 'triad',
    'hidden', 'meta', 'cross_regime', 'cross_asset', 'mtf',
    'emergent', 'adversarial', 'quantum', 'chaos', 'info_theory',
    'combinatorial', 'deep_ensemble'
]

# But implementations are stubs!
def run_chaos_suite():
    # TODO: Implement chaos theory analysis
    pass
```

### Integration Steps

#### Step 2.1: Implement Chaos Suite

```python
# File: kinetra/discovery_methods.py (EXISTS but needs completion)

import numpy as np
from scipy.spatial.distance import pdist
from scipy.stats import entropy

class ChaosTheoryAnalyzer:
    """Chaos theory metrics for market analysis."""
    
    def lyapunov_exponent(self, time_series: np.ndarray, 
                          embedding_dim: int = 3, 
                          lag: int = 1) -> float:
        """
        Calculate Lyapunov exponent (predictability measure).
        
        Positive â†’ chaotic (unpredictable)
        Zero â†’ neutral
        Negative â†’ stable (predictable)
        """
        # Time-delay embedding
        N = len(time_series)
        M = N - (embedding_dim - 1) * lag
        
        embedded = np.zeros((M, embedding_dim))
        for i in range(M):
            for j in range(embedding_dim):
                embedded[i, j] = time_series[i + j * lag]
        
        # Calculate divergence rates
        distances = pdist(embedded)
        
        if len(distances) == 0:
            return 0.0
        
        # Lyapunov exponent approximation
        lyap = np.mean(np.log(distances + 1e-10))
        return lyap
    
    def correlation_dimension(self, time_series: np.ndarray) -> float:
        """Estimate correlation dimension (fractal dimension)."""
        # ... implementation ...
        pass
    
    def permutation_entropy(self, time_series: np.ndarray, 
                            order: int = 3) -> float:
        """
        Permutation entropy (complexity measure).
        
        Low â†’ ordered
        High â†’ random/chaotic
        """
        N = len(time_series)
        permutations = []
        
        for i in range(N - order + 1):
            window = time_series[i:i+order]
            perm = tuple(np.argsort(window))
            permutations.append(perm)
        
        # Calculate entropy of permutation distribution
        unique, counts = np.unique(permutations, axis=0, return_counts=True)
        probs = counts / len(permutations)
        
        return entropy(probs)
```

#### Step 2.2: Wire into Testing Framework

```python
# File: kinetra/testing_framework.py (ADD)

from kinetra.discovery_methods import ChaosTheoryAnalyzer

def run_chaos_test(config: TestConfiguration) -> TestResult:
    """Test using chaos theory features."""
    
    analyzer = ChaosTheoryAnalyzer()
    
    # For each instrument
    results = []
    for instrument_spec in config.instruments:
        # Load data
        data = load_instrument_data(instrument_spec)
        prices = data['close'].values
        
        # Calculate chaos features
        lyapunov = analyzer.lyapunov_exponent(prices)
        perm_entropy = analyzer.permutation_entropy(prices)
        
        # Classify regime based on chaos
        if lyapunov > 0.5:
            regime = "CHAOTIC"  # Unpredictable, avoid trading
        elif lyapunov < -0.2:
            regime = "STABLE"   # Predictable, tradeable
        else:
            regime = "NEUTRAL"  # Uncertain
        
        # Create trading signal based on regime
        if regime == "STABLE":
            # Use traditional RL agent
            result = run_rl_test_on_instrument(instrument_spec, config)
        else:
            # Skip trading in chaotic regimes
            result = TestResult(skipped=True, reason=f"Chaotic (Î»={lyapunov:.2f})")
        
        results.append(result)
    
    # Aggregate results
    return aggregate_test_results(results)
```

#### Step 2.3: Create Discovery Suite Runner

```python
# File: scripts/testing/unified_test_framework.py (MODIFY)

def run_discovery_suite(suite_name: str, instruments: List[InstrumentSpec]) -> Dict:
    """Run a discovery test suite."""
    
    suite_map = {
        'chaos': run_chaos_suite,
        'hidden': run_hidden_dimensions_suite,
        'meta': run_meta_learning_suite,
        'quantum': run_quantum_inspired_suite,
        'info_theory': run_information_theory_suite,
        # ... etc
    }
    
    if suite_name not in suite_map:
        raise ValueError(f"Unknown discovery suite: {suite_name}")
    
    runner = suite_map[suite_name]
    return runner(instruments)

def run_chaos_suite(instruments: List[InstrumentSpec]) -> Dict:
    """Chaos theory test suite."""
    
    config = TestConfiguration(
        name=f"chaos_suite_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        description="Chaos theory analysis with Lyapunov exponents",
        instruments=instruments,
        agent_type='chaos',  # Custom type
        agent_config={'embedding_dim': 3, 'lag': 1},
        episodes=100,
        use_gpu=True
    )
    
    result = run_chaos_test(config)
    return result.to_dict()
```

---

## ğŸ”Œ Integration Task 3: Physics Engine â†” Test Environments

### Current Gap

Physics features computed but not consistently used in test environments:

```python
# rl_exploration_framework.py has TradingEnv but doesn't always use physics
class TradingEnv:
    def __init__(self, ...):
        # âŒ Physics engine not always initialized
        pass
    
    def step(self, action):
        # âŒ Regime not checked before allowing trades
        pass
```

### Integration Steps

#### Step 3.1: Standardize Physics Integration

```python
# File: kinetra/trading_env.py (NEW - unified environment)

from kinetra.physics_engine import PhysicsEngine
from kinetra.regime_filtered_env import RegimeFilter

class UnifiedTradingEnv(gym.Env):
    """
    Unified trading environment with physics integration.
    
    Features:
    - Physics state computation (64-dim)
    - Regime classification (laminar/chaotic/etc)
    - Regime filtering (optional, mode-dependent)
    - Multi-instrument support
    """
    
    def __init__(
        self,
        instruments: List[InstrumentSpec],
        mode: TradingMode = TradingMode.EXPLORATION,
        use_physics: bool = True,
        regime_filter: bool = False,
        **kwargs
    ):
        super().__init__()
        
        self.instruments = instruments
        self.mode = mode
        self.use_physics = use_physics
        self.regime_filter = regime_filter
        
        # Initialize physics engine
        if self.use_physics:
            self.physics_engine = PhysicsEngine()
            self.regime_filter_engine = RegimeFilter() if regime_filter else None
        
        # State space: OHLCV + physics (64) + position info
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(64 + 5 + 3,),  # physics + OHLCV + position
            dtype=np.float32
        )
        
        # Action space: [HOLD, BUY, SELL, CLOSE]
        self.action_space = gym.spaces.Discrete(4)
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, dict]:
        """Execute action and return next state."""
        
        # Get current market data
        current_bar = self.data.iloc[self.current_idx]
        
        # Compute physics state
        if self.use_physics:
            physics_state = self.physics_engine.compute_state(
                self.data.iloc[max(0, self.current_idx-100):self.current_idx+1]
            )
            regime = self.physics_engine.classify_regime(physics_state)
        else:
            physics_state = np.zeros(64)
            regime = "UNKNOWN"
        
        # Regime filter (if enabled and not in EXPLORATION mode)
        if self.regime_filter and self.mode != TradingMode.EXPLORATION:
            if not self.regime_filter_engine.is_tradeable(regime):
                # Block trade in non-tradeable regimes
                action = 0  # Force HOLD
                info = {'regime_blocked': True, 'regime': regime}
        
        # Execute trade
        reward, done, info = self._execute_action(action, current_bar)
        
        # Update state
        self.current_idx += 1
        next_state = self._get_observation()
        
        # Add physics info to metadata
        info['regime'] = regime
        info['physics_state'] = physics_state
        
        return next_state, reward, done, info
    
    def _get_observation(self) -> np.ndarray:
        """Build observation vector."""
        current_bar = self.data.iloc[self.current_idx]
        
        # OHLCV features
        ohlcv = np.array([
            current_bar['open'],
            current_bar['high'],
            current_bar['low'],
            current_bar['close'],
            current_bar['volume']
        ])
        
        # Physics features (64-dim)
        if self.use_physics:
            physics = self.physics_engine.get_features(self.current_idx)
        else:
            physics = np.zeros(64)
        
        # Position info
        position = np.array([
            self.position,  # Current position (-1, 0, 1)
            self.entry_price if self.position != 0 else 0,
            self.unrealized_pnl
        ])
        
        # Concatenate all features
        obs = np.concatenate([ohlcv, physics, position])
        return obs.astype(np.float32)
```

#### Step 3.2: Update Testing Framework to Use Unified Env

```python
# File: kinetra/testing_framework.py (MODIFY)

from kinetra.trading_env import UnifiedTradingEnv

def run_rl_test(config: TestConfiguration) -> TestResult:
    """Run RL agent test suite with unified environment."""
    
    # Create unified environment (replaces old TradingEnv)
    env = UnifiedTradingEnv(
        instruments=config.instruments,
        mode=TradingMode.EXPLORATION,  # Open for learning
        use_physics=True,  # Always use physics
        regime_filter=False,  # No filtering during exploration
        use_gpu=config.use_gpu
    )
    
    # ... rest of training loop ...
```

---

## ğŸ”Œ Integration Task 4: Results â†” Analytics Dashboard

### Current Gap

Test results saved to JSON but no visualization/analysis:

```python
# testing_framework.py saves results but doesn't analyze
results.to_json('test_results/test_YYYYMMDD_HHMMSS.json')

# âŒ No automatic analysis
# âŒ No charts/plots
# âŒ No statistical comparison
```

### Integration Steps

#### Step 4.1: Create Results Analyzer

```python
# File: kinetra/results_analyzer.py (NEW)

import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

class ResultsAnalyzer:
    """Analyze and visualize test results."""
    
    def __init__(self, results_dir: str = "test_results"):
        self.results_dir = Path(results_dir)
    
    def load_latest_results(self) -> pd.DataFrame:
        """Load most recent test results."""
        result_files = sorted(self.results_dir.glob("test_*.json"))
        if not result_files:
            raise FileNotFoundError("No test results found")
        
        latest = result_files[-1]
        with open(latest) as f:
            data = json.load(f)
        
        return pd.DataFrame(data['test_results'])
    
    def compare_suites(self, suite_names: List[str]) -> pd.DataFrame:
        """Compare performance across test suites."""
        
        results = []
        for suite in suite_names:
            suite_results = self.load_suite_results(suite)
            
            metrics = {
                'suite': suite,
                'sharpe_mean': suite_results['sharpe'].mean(),
                'sharpe_std': suite_results['sharpe'].std(),
                'omega_mean': suite_results['omega'].mean(),
                'omega_std': suite_results['omega'].std(),
                'win_rate': suite_results['win_rate'].mean(),
                'p_value': self._calculate_significance(suite_results)
            }
            results.append(metrics)
        
        return pd.DataFrame(results)
    
    def _calculate_significance(self, results: pd.DataFrame) -> float:
        """Test if results are statistically significant."""
        # One-sample t-test vs zero (no edge)
        t_stat, p_value = stats.ttest_1samp(results['sharpe'], 0)
        return p_value
    
    def plot_comparison(self, comparison_df: pd.DataFrame, 
                       output_path: str = "test_results/comparison.png"):
        """Plot suite comparison."""
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # Sharpe ratio comparison
        axes[0, 0].bar(comparison_df['suite'], comparison_df['sharpe_mean'])
        axes[0, 0].errorbar(
            comparison_df['suite'], comparison_df['sharpe_mean'],
            yerr=comparison_df['sharpe_std'], fmt='none', color='black'
        )
        axes[0, 0].set_title('Sharpe Ratio by Suite')
        axes[0, 0].set_ylabel('Sharpe Ratio')
        axes[0, 0].axhline(y=0, color='r', linestyle='--', alpha=0.3)
        
        # Omega ratio comparison
        axes[0, 1].bar(comparison_df['suite'], comparison_df['omega_mean'])
        axes[0, 1].set_title('Omega Ratio by Suite')
        axes[0, 1].set_ylabel('Omega Ratio')
        axes[0, 1].axhline(y=1, color='r', linestyle='--', alpha=0.3)
        
        # Win rate
        axes[1, 0].bar(comparison_df['suite'], comparison_df['win_rate'])
        axes[1, 0].set_title('Win Rate by Suite')
        axes[1, 0].set_ylabel('Win Rate')
        axes[1, 0].axhline(y=0.5, color='r', linestyle='--', alpha=0.3)
        
        # Statistical significance
        axes[1, 1].bar(comparison_df['suite'], -np.log10(comparison_df['p_value']))
        axes[1, 1].axhline(y=-np.log10(0.01), color='r', linestyle='--', 
                          label='p=0.01 threshold')
        axes[1, 1].set_title('Statistical Significance')
        axes[1, 1].set_ylabel('-log10(p-value)')
        axes[1, 1].legend()
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150)
        print(f"âœ… Comparison plot saved to {output_path}")
```

#### Step 4.2: Add to Test Runner

```python
# File: scripts/testing/unified_test_framework.py (MODIFY)

from kinetra.results_analyzer import ResultsAnalyzer

def main():
    # ... run tests ...
    
    # After tests complete, analyze results
    analyzer = ResultsAnalyzer()
    
    if args.compare:
        # Compare specified suites
        comparison = analyzer.compare_suites(args.compare)
        print("\n" + "="*80)
        print("SUITE COMPARISON")
        print("="*80)
        print(comparison.to_string(index=False))
        
        # Generate plots
        analyzer.plot_comparison(comparison)
        
        # Statistical summary
        winner = comparison.loc[comparison['sharpe_mean'].idxmax()]
        print(f"\nğŸ† Best Sharpe: {winner['suite']} ({winner['sharpe_mean']:.3f})")
        print(f"   p-value: {winner['p_value']:.4f}")
```

---

## ğŸ”Œ Integration Task 5: Training Scripts â†” Unified Interface

### Current Gap

Multiple training scripts with overlapping functionality:

```
scripts/training/
â”œâ”€â”€ train_rl.py
â”œâ”€â”€ train_triad.py
â”œâ”€â”€ explore_specialization.py
â”œâ”€â”€ explore_universal.py
â””â”€â”€ ... many more
```

All do similar things but differently!

### Integration Steps

#### Step 5.1: Create Unified Training Interface

```python
# File: scripts/train.py (NEW - replaces all train_*.py)

import argparse
from kinetra.testing_framework import TestConfiguration, run_test_suite
from kinetra.agent_factory import AgentFactory

def main():
    parser = argparse.ArgumentParser(description="Kinetra Unified Training Interface")
    
    # Agent type
    parser.add_argument('--agent', type=str, required=True,
                       choices=['ppo', 'dqn', 'linear_q', 'triad'],
                       help='Agent algorithm to train')
    
    # Specialization strategy
    parser.add_argument('--strategy', type=str, default='universal',
                       choices=['universal', 'asset_class', 'timeframe', 'regime'],
                       help='Specialization strategy')
    
    # Data
    parser.add_argument('--instruments', nargs='+', default=None,
                       help='Specific instruments (or auto-discover if None)')
    parser.add_argument('--asset-classes', nargs='+', default=None,
                       help='Filter by asset classes')
    parser.add_argument('--timeframes', nargs='+', default=None,
                       help='Filter by timeframes')
    
    # Training
    parser.add_argument('--episodes', type=int, default=100,
                       help='Training episodes')
    parser.add_argument('--use-gpu', action='store_true',
                       help='Use GPU acceleration')
    
    # Physics
    parser.add_argument('--use-physics', action='store_true', default=True,
                       help='Use physics engine')
    parser.add_argument('--regime-filter', action='store_true',
                       help='Filter trades by regime')
    
    # Output
    parser.add_argument('--output-dir', type=str, default='results/training',
                       help='Output directory')
    
    args = parser.parse_args()
    
    # Discover instruments
    from kinetra.data_loader import UnifiedDataLoader
    loader = UnifiedDataLoader()
    
    if args.instruments:
        instruments = loader.load_specific(args.instruments)
    else:
        instruments = loader.discover_instruments(
            asset_classes=args.asset_classes,
            timeframes=args.timeframes
        )
    
    print(f"âœ… Discovered {len(instruments)} instruments")
    
    # Create training config
    config = TestConfiguration(
        name=f"{args.agent}_{args.strategy}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        description=f"Train {args.agent} with {args.strategy} specialization",
        instruments=instruments,
        agent_type=args.agent,
        agent_config={
            'strategy': args.strategy,
            'use_physics': args.use_physics,
            'regime_filter': args.regime_filter,
        },
        episodes=args.episodes,
        use_gpu=args.use_gpu
    )
    
    # Train agent
    print(f"\nğŸš€ Training {args.agent} agent...")
    result = run_test_suite(config)
    
    # Save results
    output_file = Path(args.output_dir) / f"{config.name}.json"
    output_file.parent.mkdir(parents=True, exist_ok=True)
    result.to_json(output_file)
    
    print(f"\nâœ… Training complete!")
    print(f"   Results saved to: {output_file}")
    print(f"   Sharpe Ratio: {result.metrics['sharpe']:.3f}")
    print(f"   Omega Ratio: {result.metrics['omega']:.3f}")

if __name__ == '__main__':
    main()
```

#### Step 5.2: Usage Examples

```bash
# Train universal PPO agent
python scripts/train.py --agent ppo --strategy universal --episodes 100 --use-gpu

# Train asset-class specialists with DQN
python scripts/train.py --agent dqn --strategy asset_class --episodes 200

# Train triad system on crypto only
python scripts/train.py --agent triad --strategy universal \
    --asset-classes crypto --timeframes H1 H4

# Train with regime filtering
python scripts/train.py --agent ppo --strategy regime --regime-filter --use-physics
```

---

## ğŸ“Š Integration Validation Checklist

After completing integrations, validate with this checklist:

### âœ… Task 1: Testing Framework â†” RL Agents

- [ ] `AgentFactory` can instantiate all agent types (PPO, DQN, Linear Q, Triad)
- [ ] `run_rl_test()` successfully trains an agent for 10 episodes
- [ ] Test results include episode rewards, trade history, and metrics
- [ ] GPU acceleration works when `use_gpu=True`

### âœ… Task 2: Discovery Methods â†” Testing

- [ ] Chaos suite calculates Lyapunov exponents correctly
- [ ] Chaotic regimes are identified and handled appropriately
- [ ] Other discovery suites (hidden, meta, etc.) run without errors
- [ ] Discovery results saved to `test_results/discovery_*.json`

### âœ… Task 3: Physics Engine â†” Test Environments

- [ ] `UnifiedTradingEnv` computes physics state (64-dim)
- [ ] Regime classification works (laminar/chaotic/etc.)
- [ ] Regime filtering blocks trades when enabled
- [ ] Physics features included in observation space

### âœ… Task 4: Results â†” Analytics Dashboard

- [ ] `ResultsAnalyzer` loads and parses test results
- [ ] Suite comparison generates table and plots
- [ ] Statistical significance calculated (p-values)
- [ ] Plots saved to `test_results/comparison.png`

### âœ… Task 5: Training Scripts â†” Unified Interface

- [ ] `scripts/train.py` can train all agent types
- [ ] Instrument discovery works with filters
- [ ] Results saved in standardized format
- [ ] Command-line interface is intuitive

---

## ğŸš€ Quick Start: End-to-End Integration Test

Run this to validate complete integration:

```bash
# Step 1: Quick test with control group (baseline)
python scripts/testing/unified_test_framework.py --quick --suite control

# Step 2: Test RL agent integration
python scripts/train.py --agent ppo --strategy universal --episodes 10 --use-gpu

# Step 3: Test physics integration
python scripts/testing/unified_test_framework.py --suite physics --max-instruments 3

# Step 4: Test discovery suite
python scripts/testing/unified_test_framework.py --suite chaos --max-instruments 2

# Step 5: Compare results
python scripts/testing/unified_test_framework.py --compare control physics rl chaos

# Step 6: Check results
ls -la test_results/
cat test_results/comparison.png  # Should show comparison plots
```

**Expected Output**:
```
âœ… Control suite: MA/RSI baseline established
âœ… RL suite: PPO agent trained successfully
âœ… Physics suite: Energy/regime features computed
âœ… Chaos suite: Lyapunov exponents calculated
âœ… Comparison: Charts generated, winner identified
```

---

## ğŸ“ Summary: Integration Priorities

| Priority | Task | Effort | Impact | Status |
|----------|------|--------|--------|--------|
| ğŸ”¥ **P0** | DSP Features â†” SuperPot | 2-3 days | **PHILOSOPHY VIOLATION FIX** | âŒ TODO |
| ğŸ”¥ **P1** | Testing Framework â†” RL Agents | 2-3 days | Critical | âŒ TODO |
| ğŸ”¥ **P2** | Physics Engine â†” Environments | 1-2 days | High | âš ï¸ Partial |
| ğŸŸ¡ **P3** | Discovery Methods Implementation | 3-5 days | High | âŒ TODO |
| ğŸŸ¡ **P4** | Results â†” Analytics Dashboard | 1-2 days | Medium | âŒ TODO |
| ğŸŸ¢ **P5** | Unified Training Interface | 1 day | Medium | âŒ TODO |

**Total Estimated Effort**: 11-16 days (2-3 weeks)

**P0 is NEW and CRITICAL**: Legacy superpot violates "no fixed periods" philosophy. Must evolve to DSP-driven adaptive cycles before meaningful testing.

---

## ğŸ¯ Next Actions

1. **Start with P1**: Create `agent_factory.py` and wire RL agents
2. **Validate immediately**: Run quick test after each integration
3. **Iterate**: Fix bugs, refine interfaces
4. **Document**: Update this guide as integrations complete
5. **Test end-to-end**: Run full suite once all pieces connected

---

**END OF INTEGRATION GUIDE**

*This is a living document - update as integrations progress!*
